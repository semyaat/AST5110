{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization of a function of a single variable. Differentiation through finite differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 – Code to represent the function and the analytical and numerical derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$h(x) = \\cos\\left[\\frac{\\pi (x-1)}{2}\\right] \\exp\\left[-\\left(\\frac{x-3}{2.5}\\right)^2\\right],\\tag{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $x \\in (-4,10) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a sampling of $h$ with 64 intervals, that is, 65 points, and store the\n",
    "values into double precision arrays called $xx$ and $hh$. Numpy arrays are double precision as \n",
    "default. To define $xx$ in Python you can use the commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import numpy as np\n",
    "nump=65\n",
    "x0=-4.0 \n",
    "xf=10.0\n",
    "xx = np.arange(nump)/(nump-1.0) * (xf-x0) + x0`\n",
    "\n",
    "Use `matplotlib.pyplot` to visualize hh vs xx. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the variable $nint$ as the number of intervals ($nint= 64$ in the present case)\n",
    "and $nump$ as the number of points. In IDL, Python and C, those components go\n",
    "from the $0-$component through the component $nump−1$. Compute the ratio (1) in the [wiki](https://github.com/AST-Course/AST5110/wiki/Discretization) using and filling the function `deriv_dnw` in `nm_lib`. Feel free to use any known library or create your own functions from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will depend on how you created the function that you have $nump$ or $nump−1$ elements. If the former, the last component ($nump-1$) is ill calculated. $hp$ contains a second-order approximation to the derivative of the $hh$ function at the intermediate points $x_{i+1/2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Plot $hh$ versus $xx$ as a solid line with crosses added at each grid point (to visualize the goodness of the discretization) or with `plt.hist` function combined with `plt.plot`. _Make sure the axis pixels are properly located either to the center or half grid shifted_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from nm_lib import nm_lib as nm\n",
    "\n",
    "# plt.style.use('dark_background')\n",
    "plt.style.use('default')\n",
    "\n",
    "nump=65\n",
    "x0=-4.0 \n",
    "xf=10.0\n",
    "\n",
    "xx = np.arange(nump)/(nump-1.0) * (xf-x0) + x0\n",
    "\n",
    "def h(x): \n",
    "    # Solves h as a function of x \n",
    "    return np.cos(np.pi*(x-1) / 2) * np.exp(-((x-3) / 2.5)**2)\n",
    "\n",
    "# Visualize \n",
    "hh = h(xx)\n",
    "plt.plot(xx, hh, color='lightblue')\n",
    "plt.plot(xx, hh, 'x', color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nint = 64 # Number of intervals \n",
    "nump = 65 # Number of points \n",
    "\n",
    "plt.plot(xx, hh, color='lightblue')\n",
    "hist = plt.step((xx)+(xx[1]-xx[0])/2, hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plot the array containing the numerical derivative, $hp$. Calculate analytically the derivative of the function (1) and represent it in the same figure to ascertain the goodness of the approximation for that number of points. __hint__ _make sure the axis pixels are properly located either to the center or half grid shifted_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = nm.deriv_dnw(xx, hh) # Numerical derivative \n",
    "\n",
    "def hp_calc(x): \n",
    "    # Analytical derivative\n",
    "    return np.exp(-0.16*(-3+x)**2)  *(1.5708*np.cos((np.pi*x)/2) - (-0.96 + 0.32*x) * np.sin((np.pi*x)/2))\n",
    "\n",
    "plt.plot(xx, hp_calc(xx+(xx[1]-xx[0])/2), label='Analytical', color='b', linestyle='--')\n",
    "plt.plot(xx[:-1], hp, label='Numerical',  color='lightblue')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Repeat the foregoing, but now using $nint= 32$ and $nint= 16$ intervals to see how the approximation deteriorates. Thereafter, repeat the same process for 128 and 256 intervals, to see how it improves. Consider to use `plt.semilogy` for the error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_approx(nint, axs, semology=False): \n",
    "    # Function for visualizing the deteriation of the approximation depending on number of intervals \n",
    "    nump = nint + 1 \n",
    "    x = np.arange(nump)/(nump-1.0) * (xf-x0) + x0\n",
    "    hp = nm.deriv_dnw(x, h(x))\n",
    "\n",
    "    axs.title.set_text(f'Number of intervals: {nint}')\n",
    "    axs.scatter(x, hp_calc(x + (x[1]-x[0])/2), label='Analytical', color='b', marker='x')\n",
    "    axs.plot(x[:-1], hp,    label='Numerical',  color='lightblue')\n",
    "    axs.legend()\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(25,5))\n",
    "plot_approx(16, ax1)\n",
    "plot_approx(32, ax2)\n",
    "plot_approx(128, ax3)\n",
    "plot_approx(256, ax4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">JMS</span>.\n",
    "\n",
    "<span style=\"color:blue\">good</span>.\n",
    "\n",
    "\n",
    "<span style=\"color:red\">Please, explain why do you think that is happening and what(how) did you solve the task.</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Test of the quadratic order of the approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to test if the ratio $(h_{i+1}-h_i)/(x_{i+1}-x_i)$ approaches the analytical value of the derivative. To that end, we will use samplings with, successively, 16, 32, 64, 128, 256, 512 and 1024 intervals (which are successive powers of 2). Calculate the maximum of the absolute value of the error, meaning: the difference between the analytical and the numerical derivatives at the _same points_. Plot a graph of that value versus the size of the interval in each case using a diagram with logarithmic axes. Check if the curve you get corresponds to a quadratic dependence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error(x): \n",
    "    hp_calc_small = hp_calc(x[:-1] + (x[1]-x[0])/2)\n",
    "    hp = nm.deriv_dnw(x, h(x))\n",
    "\n",
    "    error = np.max(np.abs(hp_calc_small - hp))\n",
    "    dxs = x[1] - x[0]\n",
    "\n",
    "    return error, dxs\n",
    "\n",
    "nint       =  [16, 32, 64, 128, 256, 512, 1024]\n",
    "error_list = []\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
    "for n in nint: \n",
    "    nump = n + 1 \n",
    "    xx = np.arange(nump)/(nump-1.0) * (xf-x0) + x0\n",
    "\n",
    "    error_list.append(calc_error(xx)[0])\n",
    "\n",
    "    ax1.scatter(n, calc_error(xx)[0], label=f'{n}', marker='x')\n",
    "    ax1.legend()\n",
    "    # JMS See my comment below about the following plot\n",
    "    ax2.plot(n, calc_error(xx)) # ??? \n",
    "\n",
    "ax1.set_xlabel(\"Number of Intervals\"); ax1.set_ylabel(\"Maximum Error\")\n",
    "ax1.set_xscale('log');                 ax1.set_yscale('log')\n",
    "ax2.set_xlabel(\"Number of Intervals\"); ax2.set_ylabel(\"Maximum Error\")\n",
    "ax2.set_xscale('log');                 ax2.set_yscale('log')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">JMS</span>.\n",
    "\n",
    "<span style=\"color:red\">Please, explain why do you think that is happening and what(how) did you solve the task.</span>.\n",
    "\n",
    "<span style=\"color:red\"> I'm guessing that you are trying to do the line plot on the right figure. For this, consider the followint </span>\n",
    "\n",
    "    ax2.plot(nint, error_list) \n",
    "\n",
    "<span style=\"color:red\"> outside the loop. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Improving the accuracy of the test of the quadratic order of the approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the accuracy of the result of the previous paragraph:\n",
    "\n",
    "1. extend the test to a larger range of number of intervals (including 2048, 4096, 8192, 16384). Make sure to use double precision variables throughout the program (meaning: all variables except the array indices).\n",
    "\n",
    "2. then try to fit a straight to the logarithm of the error curves using Python program `numpy.polyfit` and `numpy.poly1d`. From the value of the slope you get from that program, check the accuracy with which you obtain the quadratic dependence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nint =  [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "error_list = []\n",
    "\n",
    "for n in nint: \n",
    "    nump = n + 1 \n",
    "    xx = np.arange(nump)/(nump-1.0) * (xf-x0) + x0\n",
    "\n",
    "    error_list.append(calc_error(xx)[0])\n",
    "\n",
    "    plt.scatter(np.log10(n), np.log10(calc_error(xx)[0]), label=f'{n}', marker='x')\n",
    "    plt.legend()\n",
    "\n",
    "p = np.poly1d(np.polyfit(np.log10(nint), np.log10(error_list), deg=1))\n",
    "plt.plot(np.log10(nint), p(np.log10(nint)), color='lightblue', linestyle='--')\n",
    "\n",
    "plt.xlabel(\"Number of Intervals\"); plt.ylabel(\"Maximum Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final three points are not behaving linearly, so we try dropping these from the interpolation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in nint: \n",
    "    nump = n + 1 \n",
    "    xx = np.arange(nump)/(nump-1.0) * (xf-x0) + x0\n",
    "    plt.scatter(np.log10(n), np.log10(calc_error(xx)[0]), label=f'{n}', marker='x')\n",
    "    plt.legend()\n",
    "\n",
    "p = np.poly1d(np.polyfit(np.log10(nint[:-3]), np.log10(error_list[:-3]), deg=1))\n",
    "plt.plot(np.log10(nint), p(np.log10(nint)), color='lightblue', linestyle='--')\n",
    "\n",
    "plt.xlabel(\"Number of Intervals\"); plt.ylabel(\"Maximum Error\")\n",
    "\n",
    "slope = p.coef[0] # Get the slope \n",
    "print(slope)\n",
    "print(f\"Error of quadratic dependence: {np.abs(np.sum(error_list)/len(error_list) - slope):.1e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">JMS</span>.\n",
    "\n",
    "<span style=\"color:blue\">good</span>.\n",
    "\n",
    "\n",
    "<span style=\"color:red\">Please, explain why do you think that is happening and what(how) did you solve the task.</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Analytical proof of the order of convergence of the approximation for the derivative (optional)\n",
    "\n",
    "Consider the sampling used in exercise this, assuming that the spacing between grid points is uniform, i.e., $(\\Delta x)_i = \\Delta x$. Write a formal Taylor expansion as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x_{i+1}) = f(x_{i+1/2}) + f'(x_{i+1/2})\\frac{\\Delta x}{2} + ...  \\tag{2}$$\n",
    "\n",
    "$$f(x_{i}) = f(x_{i+1/2}) - f'(x_{i+1/2})\\frac{\\Delta x}{2} + ...  \\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "including terms up to order $(\\Delta x)^3$. Eliminating terms combining those two expressions, conclude that, as said in the previous exercise sheet, the finite-difference approximation to the derivative at the midpoints $x_{i+1/2}$ carried out there is of 2nd order."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">JMS</span>.\n",
    "\n",
    "<span style=\"color:yellow\">optional, but it would be nice if you can add some sentences, not necessarily to do all the derivations </span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
